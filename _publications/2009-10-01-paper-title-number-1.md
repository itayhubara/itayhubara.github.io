---
title: "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators"
collection: publications
permalink: /publication/2009-10-01-paper-title-number-1
excerpt: 'The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, -bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy.'
date: 2023-12
venue: 'WANT@NeurIPS'
paperurl: '[WANT@NeurIPS](https://openreview.net/forum?id=wMbe8fVjgf)'
citation: 'Yaniv.B, Itay.H, Daniel.S (2023). &quot; Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators.&quot; <i>WANT@NeurIPS</i>. 1(1).'
---
This paper is about the number 1. The number 2 is left for future work.

[Download paper here](https://openreview.net/pdf?id=wMbe8fVjgf)

Recommended citation: Yaniv.B, Itay.H, Daniel.S (2023). "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators." <i>WANT@NeurIPS</i>. 1(1).
