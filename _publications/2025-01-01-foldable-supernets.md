---
title: "Foldable SuperNets: Scalable Merging of Transformers with Different Initializations and Tasks"
collection: publications
permalink: /publication/2025-01-01-foldable-supernets
excerpt: 'We propose Foldable SuperNet (FoldSN), a novel method for merging multiple Transformer models trained on different tasks and initializations into a single, scalable SuperNet. This approach enables dynamic resource allocation and efficient multi-task inference.'
date: 2025-01-01
venue: 'Transactions on Machine Learning Research (TMLR)'
paperurl: 'https://openreview.net/pdf?id=6FqwLestHv'
citation: 'Edan Kinderman, Itay Hubara, Haggai Maron, Daniel Soudry. (2025). &quot;Foldable SuperNets: Scalable Merging of Transformers with Different Initializations and Tasks.&quot; <i>TMLR 2025</i>.'
---
We propose Foldable SuperNet (FoldSN), a novel method for merging multiple Transformer models trained on different tasks and initializations into a single, scalable SuperNet. This approach enables dynamic resource allocation and efficient multi-task inference.

[Download paper here](https://openreview.net/pdf?id=6FqwLestHv)
