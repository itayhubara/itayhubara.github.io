---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Education
======
* B.Sc. in Electrical Engineering, Technion Israel Institude of Technology
* M.Sc. in Electrical Engineering (summa cum laude), Technion Israel Institude of Technology
* Ph.D iin Electrical Engineering, Technion Israel Institude of Technology
* Thesis title: Toward Fast and efficent Deep Learning

Work experience
======
* 2022- present Habana Labs and Intel; company, Principal engineer, Senior Researcher
  * 2016-2021 Habana Labs, Senior Researcher (US branch)
  • Research and development- focused on Compressing and speeding up Deep Neural Network algorithms.
  • Leading Habana’s MLPerf submissions and benchmarking efforts.
  • Technical lead and mentor for several members of the algorithm and workload analysis teams
* 2014-2016 Intel LTD, Algorithm Engineer - Computer Vision Team
  Research and development- focused on Deep Neural Network algorithms (CNN, RNN) and global path planning in
  high dimensional space algorithms
* 2014-2016 Technion - Israel Institute of Technology, Teaching Assistant & B.Sc. Final Project Supervisor
  *Taught Machine learning/image processing courses and labs to undergraduate students and supervised deep learning-
  oriented B.Sc. projects

* 2014-2015 AerialGuard, Algorithm Expert – External Advisor
  Created a widely used fast collision avoidance algorithm with low time complexity
* 2010-2014 CSR Group LTD, Algorithm Engineer - Computer Vision Team
  Research and development- focused on DFD (Depth From De-focus), object detection and tracking
  
Skills
======
* Algorithm
  * Machine learning/ Deep learning
  * Nural Languge Processing (LLM,
  * Computer vision (Detection,segmentation recognition) 
  * Optimization
  * Combinatorical problems (Integer\Linear) programing
* Coding
  * Python, Pytorch, TF, Cuda, C++
  * Github fouent 
* MLPerf
  * Benchmark owner 
  * Leading Habana efforts and representing the company in all MLPerf meetings


Awards
======
2021 - MLIS scholarship for excellent graduate student in data science.
2019 – ICML top 5% reviewers award
2018 – Jury Award for excellent graduate student
2017 – AI Grant 1.0
2017 – MSc Electrical Engineering with highest honor – summa cum laude
  
Publications
======
Publications
• Chmiel, B., Hubara, I., Banner, R., & Soudry, D. (2022). Optimal Fine-Grained N: M sparsity for
Activations and Neural Gradients. ICLR 2023.
• Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, S., Soudry, D., Accelerated Sparse Neural Training: A
Provable and Efficient Method to Find N: M Transposable Masks. NeurIPS 2021
• Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., & Soudry, D. Accurate Post Training Quantization With
Small Calibration Sets. ICML 2021
• Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoefler, T., & Soudry, D. Augment Your Batch: Improving
Generalization Through Instance Repetition. CVPR 2020

• Haroush, Matan, Itay Hubara, Elad Hoffer, and Daniel Soudry. "The knowledge within: Methods for data-
free model compression." CVPR 2020.

• Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural
networks (NIPS 2018)
• Elad Hoffer, Itay Hubara, and Daniel Soudry. Fix your classifier:the marginal value of training the last
weight layer (ICLR 2018)

• Hoffer, E., Hubara I., & Soudry, C. D. Train longer, generalize better: closing the generalization gap in
large batch training of neural networks (NIPS 2017)
• Hubara I., Courbariaux, M., Soudry, C. D., El-Yaniv, R., & Bengio, Y. Quantized Neural Networks:
Training Neural Networks with Low Precision Weights and Activations (Accepted to JMLR 2017)
• Hubara I., Courbariaux, M., Soudry, C. D., El-Yaniv, R., & Bengio, Y. Binarized Neural Networks: Training
Neural Networks with Weights and Activations Constrained to + 1 or −1 (NIPS 2016)
• Bhonker, Nadav, Shai Rozenberg, and Itay Hubara. Playing SNES in the Retro Learning
Environment. (ICLR 2016 workshop).
• Soudry, D., Hubara, I., & Meir, R. (2014). Expectation backpropagation: parameter-free training of
multilayer neural networks with continuous or discrete weights. (NIPS 2014)
• Additional publication can be found at: https://scholar.google.com/citations?user=dyYryZYAAAAJ&hl=en
  
Patents
======
• Hubara, Itay. Large-scale computations using an adaptive numerical format. U.S. Patent No. 10,491,239. 26
Nov. 2019.
• El-Yaniv, Ran, Hubara, Itay, and Soudry Daniel. Quantized neural network training and inference. U.S.
Patent Application No. 15/478,531.
  

