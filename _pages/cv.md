---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Education
======
* B.Sc. in Electrical Engineering, Technion Israel Institude of Technology
* M.Sc. in Electrical Engineering (summa cum laude), Technion Israel Institude of Technology
* Ph.D iin Electrical Engineering, Technion Israel Institude of Technology
* Thesis title: Toward Fast and efficent Deep Learning

Work experience
======
* 2022- present Habana Labs and Intel; company, Principal engineer, Senior Researcher
  * 2016-2021 Habana Labs, Senior Researcher (US branch)
  • Research and development- focused on Compressing and speeding up Deep Neural Network algorithms.
  • Leading Habana’s MLPerf submissions and benchmarking efforts.
  • Technical lead and mentor for several members of the algorithm and workload analysis teams
* 2014-2016 Intel LTD, Algorithm Engineer - Computer Vision Team
  Research and development- focused on Deep Neural Network algorithms (CNN, RNN) and global path planning in
  high dimensional space algorithms
* 2014-2016 Technion - Israel Institute of Technology, Teaching Assistant & B.Sc. Final Project Supervisor
  *Taught Machine learning/image processing courses and labs to undergraduate students and supervised deep learning-
  oriented B.Sc. projects

* 2014-2015 AerialGuard, Algorithm Expert – External Advisor
  Created a widely used fast collision avoidance algorithm with low time complexity
* 2010-2014 CSR Group LTD, Algorithm Engineer - Computer Vision Team
  Research and development- focused on DFD (Depth From De-focus), object detection and tracking
  
Skills
======
* Algorithm
  * Machine learning/ Deep learning
  * Neural Languge Processing: LLMs starting from older versions sucha as LSTM to the newer decoder basde models (Llama/MoE/GPT)
  * Computer vision: Detection,segmentation recognition
  * Optimization: Quantization (FP8/FP4), sparsification (block-sparse,unstructured), LLMs inference acceleration serving solutions (vLLM/TGI)
  * Combinatorical problems (Integer\Linear) programing
* Coding
  * Python, Pytorch, TF, Cuda, C++
  * Github fouent 
* MLPerf (training/inference)
  * Benchmark owner (old SSD/ Llama-70B-Lora)
  * Leading Habana efforts and representing the company in all MLPerf meetings


Awards
======
2021 - MLIS scholarship for excellent graduate student in data science.
2019 – ICML top 5% reviewers award
2018 – Jury Award for excellent graduate student
2017 – AI Grant 1.0
2017 – MSc Electrical Engineering with highest honor – summa cum laude
  
Publications
======
E Kinderman, **I Hubara**, H Maron, D Soudry Foldable _SuperNets: Scalable Merging of Transformers with Different Initializations and Tasks._ arxiv preprint 2024

Y Blumenfeld, **I Hubara**, D Soudry _Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators_ ICLR 2024

• Chmiel, B., **Hubara, I.**, Banner, R., & Soudry, D. (2022). _Optimal Fine-Grained N: M sparsity for
Activations and Neural Gradients_. ICLR 2023.

• **Hubara, I.**, Chmiel, B., Island, M., Banner, R., Naor, S., Soudry, D., _Accelerated Sparse Neural Training: A
Provable and Efficient Method to Find N: M Transposable Masks._ NeurIPS 2021

• **Hubara, I.**, Nahshan, Y., Hanani, Y., Banner, R., & Soudry, D. A_ccurate Post Training Quantization With
Small Calibration Sets._ ICML 2021

• Hoffer, E., Ben-Nun, T., **Hubara, I**., Giladi, N., Hoefler, T., & Soudry, D. _Augment Your Batch: Improving
Generalization Through Instance Repetition_. CVPR 2020

• Haroush, Matan, **Itay Hubara**, Elad Hoffer, and Daniel Soudry. _The knowledge within: Methods for data-
free model compression._ CVPR 2020.

• Ron Banner, **Itay Hubara**, Elad Hoffer, and Daniel Soudry. _Scalable methods for 8-bit training of neural
networks_ (NIPS 2018)

• Elad Hoffer, **Itay Hubara**, and Daniel Soudry. _Fix your classifier:the marginal value of training the last
weight layer_ (ICLR 2018)

• Hoffer, E., **Hubara I.**, & Soudry, C. D. T_rain longer, generalize better: closing the generalization gap in
large batch training of neural networks_ (NIPS 2017)

• **Hubara I.**, Courbariaux, M., Soudry, C. D., El-Yaniv, R., & Bengio, Y. _Quantized Neural Networks:
Training Neural Networks with Low Precision Weights and Activations_ (JMLR 2017)

• **Hubara I.**, Courbariaux, M., Soudry, C. D., El-Yaniv, R., & Bengio, Y. _Binarized Neural Networks: Training
Neural Networks with Weights and Activations Constrained to + 1 or −1_ (NIPS 2016)

• Bhonker, Nadav, Shai Rozenberg, and **Itay Hubara**. _Playing SNES in the Retro Learning
Environment_. (ICLR 2016 workshop).

• Soudry, D., **Hubara, I.**, & Meir, R. (2014). _Expectation backpropagation: parameter-free training of
multilayer neural networks with continuous or discrete weights._ (NIPS 2014)

• Additional publication can be found at: https://scholar.google.com/citations?user=dyYryZYAAAAJ&hl=en
  
Patents
======
• Hubara, Itay. Large-scale computations using an adaptive numerical format. U.S. Patent No. 10,491,239. 26
Nov. 2019.
• El-Yaniv, Ran, Hubara, Itay, and Soudry Daniel. Quantized neural network training and inference. U.S.
Patent Application No. 15/478,531.
  

